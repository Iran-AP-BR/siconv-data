{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c602b3a-a3f5-4b68-8c1b-fd4ba39b079c",
   "metadata": {},
   "source": [
    "\n",
    "# Modelagem de um classificador binário de convenios  \n",
    "\n",
    "Todos os convênios que não estavam na situação 'em execução' foram classificados entre insucesso e não-insucesso.  \n",
    "Foram classifcados como insucesso os convênios cuja situação era 'Prestação de Contas Rejeitada', 'Inadimplente', 'Convênio Rescindido' os demais foram classificados com não-insucesso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdb4a0b-d487-4033-b571-a34a093f2108",
   "metadata": {},
   "source": [
    "## 1. Importando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b206ab-84f1-4d94-8d6e-094136116321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "\n",
    "import nltk\n",
    "_ = nltk.download('rslp', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42335d-d37c-411f-ae04-2b2068975d17",
   "metadata": {},
   "source": [
    "## 2. Caregamento dos dados brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fdf510-4b32-4408-9ea3-819b211b167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convenios = pd.read_csv('../data_files/csv_files/convenios.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "proponentes = pd.read_csv('../data_files/csv_files/proponentes.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "emendas_convenios = pd.read_csv('../data_files/csv_files/emendas_convenios.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "emendas = pd.read_csv('../data_files/csv_files/emendas.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "movimento = pd.read_csv('../data_files/csv_files/movimento.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "fornecedores = pd.read_csv('../data_files/csv_files/fornecedores.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699e1df-64ce-4ea7-b027-49853462ed73",
   "metadata": {},
   "source": [
    "## 3. Seleção dos convênios\n",
    "> Apenas convenios que não estão em execução e cujo ano do fim da vigência é 2017 ou posterior. \n",
    ">\n",
    "> Foram considerados como insucesso convênios cuja situação é 'Prestação de Contas Rejeitada', 'Inadimplente' ou 'Convênio Rescindido'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9c3d1-f247-4a4c-82b9-e2ba10da0bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "convenios = convenios[(convenios['DIA_FIM_VIGENC_CONV'].astype('datetime64[ns]').dt.year >=2017) & (convenios['DIA_PUBL_CONV'].notna())].copy()\n",
    "convenios = convenios[convenios['SIT_CONVENIO'].str.upper()!='EM EXECUÇÃO']\n",
    "convenios['INSUCESSO'] = 0\n",
    "convenios.loc[convenios['SIT_CONVENIO'].isin(['Prestação de Contas Rejeitada', 'Inadimplente', 'Convênio Rescindido']), 'INSUCESSO'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03c242-987f-4c51-ae5e-4985e5638395",
   "metadata": {},
   "source": [
    "## 4. Definição da classe TextTransformer  \n",
    "> Esta classe realiza a o agrupamento (clustering) de textos com o algorítimo K-Means.  \n",
    "> A finalidade aqui é representar os textos da coluna 'OBJETO_PROPOSTA' em classes designadas por números inteiros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6415766-8dd4-4fb7-b6fa-5d7261dde0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_clusters=5, stop_words=[], accented=[]):\n",
    "        \n",
    "        self.__n_clusters__ = n_clusters\n",
    "        self.__stop_words__ = stop_words\n",
    "        self.__accented__ = accented\n",
    "        self.labels_ = None\n",
    "        self.__vectorizer__ = TfidfVectorizer(use_idf=True)\n",
    "        self.__clusterer__ = KMeans(n_clusters=self.__n_clusters__, random_state=0)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.fit_transform(X)\n",
    "        self.__clusterer__ = self.__clusterer__.fit(X)\n",
    "        self.labels_ = self.__clusterer__.labels_\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.fit_transform(X)\n",
    "        X = self.__clusterer__.fit_transform(X)\n",
    "        self.labels_ = self.__clusterer__.labels_\n",
    "        return X\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.fit_transform(X)\n",
    "        y = self.__clusterer__.fit_predict(X)\n",
    "        self.labels_ = self.__clusterer__.labels_\n",
    "        return y\n",
    "    \n",
    "    def transform(self, X):        \n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.transform(X)\n",
    "        X = self.__clusterer__.transform(X)\n",
    "        return X\n",
    "                       \n",
    "    def predict(self, X):\n",
    "        \n",
    "        return_unique = False\n",
    "        if type(X)==str:\n",
    "            X = [X]\n",
    "            return_unique = True\n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.transform(X)\n",
    "        y = self.__clusterer__.predict(X)\n",
    "        y = y[0] if return_unique else y\n",
    "        return y\n",
    "    \n",
    "    def __preprocessing__(self, X):\n",
    "        \n",
    "        X = [' '.join(self.__text_preprocessing__(t)) for t in X]\n",
    "        return X\n",
    "    \n",
    "    def __stemming__(self, tokens):  \n",
    "        \n",
    "        stemmer = nltk.stem.RSLPStemmer()\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def __remove_accents__(self, text):\n",
    "        \n",
    "        for idx in self.__accented__.index:\n",
    "            text = text.replace(self.__accented__['char_acc'][idx], self.__accented__['char_norm'][idx]) \n",
    "        return text\n",
    "\n",
    "    def __text_preprocessing__(self, text):\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = self.__remove_accents__(text)\n",
    "        tokens = re.findall('[a-z]+', text)\n",
    "        tokens = filter(lambda w: w is not None, map(lambda w: None if w in self.__stop_words__ or len(w)==1 else w , tokens))\n",
    "        tokens = self.__stemming__(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342e33bd-ef73-4586-975f-48e6dbfe6209",
   "metadata": {},
   "source": [
    "## 5. Definição da função transform_dataset  \n",
    "> Esta função realiza a conversão dos dados distribuídos nas tabelas (dataframes carregados) para um dataset (conjunto de dados).\n",
    "> A finalidade aqui obter um conjunto de dados no formato apropriado para os algorítimos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de7f031-6d00-48b6-a009-779596cd4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(convenios, proponentes, emendas, emendas_convenios, fornecedores, movimento, ylabel=False, ylabel_name='INSUCESSO'):\n",
    "\n",
    "    ibge = proponentes[['IDENTIF_PROPONENTE', 'CODIGO_IBGE']].copy()\n",
    "\n",
    "    selected_columns = ['VL_REPASSE_CONV', 'VL_CONTRAPARTIDA_CONV', 'VALOR_EMENDA_CONVENIO',\n",
    "           'OBJETO_PROPOSTA', 'COD_ORGAO', 'COD_ORGAO_SUP', 'NATUREZA_JURIDICA',\n",
    "           'MODALIDADE', 'IDENTIF_PROPONENTE', 'COM_EMENDAS']\n",
    "\n",
    "    features_columns = ['NR_CONVENIO', *selected_columns]\n",
    "    if ylabel:\n",
    "        features_columns += [ylabel_name]\n",
    "\n",
    "    convenios_ = convenios[features_columns].copy()\n",
    "\n",
    "    principais_parlamentares = get_principais_parlamentares(emendas=emendas, emendas_convenios=emendas_convenios, convenios_list=convenios_['NR_CONVENIO'].to_list())\n",
    "    principais_fornecedores = get_principais_fornecedores(movimento=movimento, fornecedores=fornecedores, convenios_list=convenios_['NR_CONVENIO'].to_list())\n",
    "\n",
    "    dataset = pd.merge(convenios_, ibge, how='inner', on=['IDENTIF_PROPONENTE'], left_index=False, right_index=False)\n",
    "\n",
    "    dataset = pd.merge(dataset, principais_parlamentares, how='left', on=['NR_CONVENIO'], left_index=False, right_index=False)\n",
    "\n",
    "    dataset = pd.merge(dataset, principais_fornecedores, how='left', on=['NR_CONVENIO'], left_index=False, right_index=False)\n",
    "\n",
    "    dataset = dataset.fillna('NAO APLICAVEL')\n",
    "\n",
    "    Xdtypes = {'VL_REPASSE_CONV': 'float64', 'VL_CONTRAPARTIDA_CONV': 'float64', \n",
    "               'VALOR_EMENDA_CONVENIO': 'float64', 'OBJETO_PROPOSTA': 'object', \n",
    "               'COD_ORGAO': 'int64', 'COD_ORGAO_SUP': 'int64', 'NATUREZA_JURIDICA': 'object', \n",
    "               'MODALIDADE': 'object', 'IDENTIF_PROPONENTE': 'int64', 'COM_EMENDAS': 'object',\n",
    "               'CODIGO_IBGE': 'int64', 'PRINCIPAL_PARLAMENTAR': 'object', 'PRINCIPAL_FORNECEDOR': 'object'}\n",
    "\n",
    "    return dataset.astype(Xdtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45d05aa-f9ab-491e-a10b-567b2439ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_principais_parlamentares( emendas, emendas_convenios, convenios_list):\n",
    "\n",
    "    convenios_repasses_emendas = emendas_convenios.loc[emendas_convenios['NR_CONVENIO'].isin(convenios_list)].copy()\n",
    "    convenios_repasses_emendas['rank'] = convenios_repasses_emendas.groupby(by=['NR_CONVENIO'])['VALOR_REPASSE_EMENDA'].rank(ascending=False, method='min')\n",
    "    convenios_repasses_emendas = convenios_repasses_emendas.loc[convenios_repasses_emendas['rank']==1, ['NR_CONVENIO', 'NR_EMENDA']]\n",
    "    convenios_parlamentares = pd.merge(convenios_repasses_emendas, emendas, on=['NR_EMENDA'], left_index=False, right_index=False)\n",
    "    convenios_parlamentares = convenios_parlamentares[['NR_CONVENIO', 'NOME_PARLAMENTAR']]\n",
    "    convenios_parlamentares.columns = ['NR_CONVENIO', 'PRINCIPAL_PARLAMENTAR']\n",
    "    convenios_parlamentares = convenios_parlamentares.groupby(by=['NR_CONVENIO']).max().reset_index()\n",
    "\n",
    "    return convenios_parlamentares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175759b-caf6-4acc-8ac2-9a91a72f517d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_principais_fornecedores(movimento, fornecedores, convenios_list):\n",
    "\n",
    "    movimento_exec = movimento.loc[movimento['NR_CONVENIO'].isin(convenios_list)].copy()\n",
    "    movimento_exec = movimento_exec[movimento_exec['TIPO_MOV']=='P']\n",
    "    convenios_fornecedores = movimento_exec[['NR_CONVENIO', 'FORNECEDOR_ID', 'VALOR_MOV']].groupby(by=['NR_CONVENIO', 'FORNECEDOR_ID']).sum().reset_index().copy()\n",
    "    convenios_fornecedores['rank'] = convenios_fornecedores.groupby(by=['NR_CONVENIO'])['VALOR_MOV'].rank(ascending=False, method='min')\n",
    "    convenios_fornecedores = convenios_fornecedores.loc[convenios_fornecedores['rank']==1, ['NR_CONVENIO', 'FORNECEDOR_ID']]\n",
    "    convenios_fornecedores = pd.merge(convenios_fornecedores, fornecedores, on=['FORNECEDOR_ID'], left_index=False, right_index=False)\n",
    "    convenios_fornecedores = convenios_fornecedores.sort_values(['NR_CONVENIO', 'IDENTIF_FORNECEDOR'], ascending=False)\n",
    "    convenios_fornecedores = convenios_fornecedores[['NR_CONVENIO', 'IDENTIF_FORNECEDOR']]\n",
    "    convenios_fornecedores.columns = ['NR_CONVENIO', 'PRINCIPAL_FORNECEDOR']\n",
    "    convenios_fornecedores = convenios_fornecedores.groupby(by=['NR_CONVENIO']).max().reset_index()\n",
    "\n",
    "    return convenios_fornecedores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef2858-4973-4adc-b4ae-35c1b9b07371",
   "metadata": {},
   "source": [
    "## 6. Definição da função make_transformers  \n",
    "> Esta função prepara um conjunto transformadores (transformers) para converter e normalizar os dados.   \n",
    "> 1. Para os dados categóricos foi utilizado o codificador (encoder) 'OneHotEncoder'.  \n",
    "> 2. Os dados contínuos foram normalizados com o 'PowerTransformer'.  \n",
    ">\n",
    "> A função make_transformers recebe um dataframe com os dados brutos e retorna um dicionário contendo todos os transformadores treinados.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b899f8a3-12b7-49a8-b25e-3bf9487c2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transformers(X, stop_words_path, accented_path):\n",
    "\n",
    "    accented = pd.read_csv(accented_path, compression='gzip', sep=';', encoding='utf-8')\n",
    "    stop_words = pd.read_csv(stop_words_path, compression='gzip', encoding='utf-8', header=None)[0].tolist()\n",
    "    data = X.copy()\n",
    "    text_clusterer = TextTransformer(n_clusters=50, stop_words=stop_words, accented=accented).fit(data['OBJETO_PROPOSTA'])\n",
    "    data['OBJETO_PROPOSTA'] = text_clusterer.predict(data['OBJETO_PROPOSTA'])\n",
    "    data['OBJETO_PROPOSTA'] = data['OBJETO_PROPOSTA'].astype('int64')\n",
    "\n",
    "    data_categorical_parlamentar = data.pop('PRINCIPAL_PARLAMENTAR').to_frame()\n",
    "    data_categorical_fornecedor = data.pop('PRINCIPAL_FORNECEDOR').to_frame()\n",
    "\n",
    "    data_categorical_object = data.select_dtypes(include=['object'])\n",
    "    data_categorical_int = data.select_dtypes(include=['int64'])\n",
    "    data_value = data.select_dtypes(include='float64')\n",
    "\n",
    "    transformers = {}\n",
    "    transformers['TEXT_CLUSTERER'] = text_clusterer\n",
    "    transformers['VALUE'] = PowerTransformer().fit(data_value)\n",
    "    transformers['CATEGORICAL_OBJECT'] = OneHotEncoder(handle_unknown='ignore').fit(data_categorical_object)\n",
    "    transformers['CATEGORICAL_INT'] = OneHotEncoder(handle_unknown='infrequent_if_exist', max_categories=500).fit(data_categorical_int)\n",
    "    transformers['CATEGORICAL_PARLAMENTAR'] = OneHotEncoder(handle_unknown='infrequent_if_exist', max_categories=500).fit(data_categorical_parlamentar)\n",
    "    transformers['CATEGORICAL_FORNECEDOR'] = OneHotEncoder(handle_unknown='infrequent_if_exist', max_categories=500).fit(data_categorical_fornecedor)\n",
    "\n",
    "    return transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dbf406-c06a-40ef-8f9b-a5d8dfc466d8",
   "metadata": {},
   "source": [
    "## 7. Definição da função data_preparation  \n",
    "> Esta função utiliza os transformadores (transformers) para converter e normalizar os dados.  \n",
    "> A finalidade aqui é adequar os dados para serem utilizados nos algorítimos.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa302a-e68d-4c6c-a780-8c87076e2d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(transformers, X):\n",
    "\n",
    "    data = X.copy()\n",
    "    #transformers = transformers\n",
    "    data['OBJETO_PROPOSTA'] = transformers['TEXT_CLUSTERER'].predict(data['OBJETO_PROPOSTA'])\n",
    "    data['OBJETO_PROPOSTA'] = data['OBJETO_PROPOSTA'].astype('int64')\n",
    "\n",
    "    data_categorical_parlamentar = data.pop('PRINCIPAL_PARLAMENTAR').to_frame()\n",
    "    data_categorical_fornecedor = data.pop('PRINCIPAL_FORNECEDOR').to_frame()\n",
    "\n",
    "    data_categorical_object = data.select_dtypes(include=['object'])\n",
    "    data_categorical_int = data.select_dtypes(include=['int64'])\n",
    "    data_value = data.select_dtypes(include='float64')\n",
    "\n",
    "    value_codes = transformers['VALUE'].transform(data_value)\n",
    "    value_feature_names = transformers['VALUE'].feature_names_in_\n",
    "    data_value = pd.DataFrame(value_codes, columns=value_feature_names).astype('float64')\n",
    "\n",
    "    categorical_object_codes = transformers['CATEGORICAL_OBJECT'].transform(data_categorical_object).toarray()\n",
    "    categorical_object_feature_names= transformers['CATEGORICAL_OBJECT'].get_feature_names_out()\n",
    "    data_categorical_object = pd.DataFrame(categorical_object_codes, columns=categorical_object_feature_names).astype('float64')\n",
    "\n",
    "    categorical_int_codes = transformers['CATEGORICAL_INT'].transform(data_categorical_int).toarray()\n",
    "    categorical_int_feature_names= transformers['CATEGORICAL_INT'].get_feature_names_out()\n",
    "    data_categorical_int = pd.DataFrame(categorical_int_codes, columns=categorical_int_feature_names).astype('float64')\n",
    "\n",
    "    parlamentar_codes = transformers['CATEGORICAL_PARLAMENTAR'].transform(data_categorical_parlamentar).toarray()\n",
    "    parlamentar_feature_names= transformers['CATEGORICAL_PARLAMENTAR'].get_feature_names_out()\n",
    "    data_categorical_parlamentar = pd.DataFrame(parlamentar_codes, columns=parlamentar_feature_names).astype('float64')\n",
    "\n",
    "    fornecedor_codes = transformers['CATEGORICAL_FORNECEDOR'].transform(data_categorical_fornecedor).toarray()\n",
    "    fornecedor_feature_names= transformers['CATEGORICAL_FORNECEDOR'].get_feature_names_out()\n",
    "    data_categorical_fornecedor = pd.DataFrame(fornecedor_codes, columns=fornecedor_feature_names).astype('float64')\n",
    "\n",
    "    return pd.concat([data_value, data_categorical_object, data_categorical_int,\n",
    "                      data_categorical_parlamentar, data_categorical_fornecedor], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b33a48a-a860-43b7-8944-6a892cac90ae",
   "metadata": {},
   "source": [
    "## 8. Definição da função make_train_test_base  \n",
    "> Esta função divide os dados originais em duas partes: uma para treino e outra para testes (avaliação).  \n",
    "> A finalidade aqui é deixar essas bases disponíveis para os procedimwentos de treino e avaliação.  \n",
    "> Esta função também se encarrega da conversão da coluna 'OBJETO_PROPOSTA' em agrupamentos (clusters), o que a transforma em uma coluna com dados categóricos passíveis de serem manipulados pelos algorítimos de normalização e classificação.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6269702-f25d-41e4-ad40-d907daee1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_bases(**tables):\n",
    "    \n",
    "    assert not tables or set(tables.keys()) == {'convenios', 'proponentes', 'emendas', 'emendas_convenios', \n",
    "    'fornecedores', 'movimento'}, '''Named arguments, if provided, must be: convenios, emendas, emendas_convenios, fornecedores, movimento'''\n",
    "\n",
    "    ylabel_name='INSUCESSO'\n",
    "    print('transforming tables into dataset ... ', end='')\n",
    "    data = transform_dataset(**tables, ylabel=True)\n",
    "    data = data.drop(['NR_CONVENIO'], axis=1)\n",
    "    print('ok')\n",
    "\n",
    "    print('balancing ... ', end='')\n",
    "    data = data.sample(frac=1).reset_index(drop=True)\n",
    "    q0 = len(data[data[ylabel_name]==0])\n",
    "    q1 = len(data[data[ylabel_name]==1])\n",
    "    q = q0 if q0<q1 else q1\n",
    "\n",
    "    X = pd.concat([data[data[ylabel_name]==0].iloc[0:q], data[data[ylabel_name]==1].iloc[0:q]], sort=False)\n",
    "    y = X[[ylabel_name]]\n",
    "    X = pd.concat([data[data[ylabel_name]==0].iloc[0:q], data[data[ylabel_name]==1].iloc[0:q]], sort=False)\n",
    "    rest = data[~data.index.isin(X.index)]\n",
    "    print('ok')\n",
    "\n",
    "    print('spliting ... ', end='')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "    print('ok')\n",
    "\n",
    "    print('saving train and test datasets ... ', end='')\n",
    "    X_train.to_csv('./datasets/convenios_train.tsv.gz', compression='gzip', sep='\\t', encoding='utf-8', index=False)\n",
    "    X_test.to_csv('./datasets/convenios_test.tsv.gz', compression='gzip', sep='\\t', encoding='utf-8', index=False)\n",
    "    print('ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68907744-8a88-4665-b2f3-fe8d8a06676b",
   "metadata": {},
   "source": [
    "## 9. Definição da função get_tuned_estimator  \n",
    "> Esta função realiza a seleção de hiperparâmetros (tuning).  \n",
    "> A finalidade aqui é selecionar os melhores dentre um conjunto de hiperparâmetros escolhidos para o modelo informado.  \n",
    "> Esta função retorna o melhor score obtido e os hiperparâmetros correspondentes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5fed6-885d-4418-80be-dd6990182f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_estimator(X, y, estimator, param_dist, scoring='accuracy'):\n",
    "    rand = GridSearchCV(estimator, param_dist, cv=10, refit=True, scoring=scoring)\n",
    "    rand.fit(X, y['INSUCESSO'])\n",
    "\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ce477-aeab-4345-9529-499a0c102d14",
   "metadata": {},
   "source": [
    "## 10. Definição da função load_bases  \n",
    "> Esta função é utilizada para a carga das bases de treino e de teste.  \n",
    "> A finalidade aqui é propiciar um meio único e confiável de se realizar o carregamento das bases de treino e de teste.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a371ef-9e31-4061-88ba-8ed4e03d4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bases(type='both', ylabel_name='INSUCESSO'):\n",
    "\n",
    "    assert type in ['train', 'test', 'both']\n",
    "\n",
    "    result = []\n",
    "    if type.lower() in ['train', 'both']:\n",
    "        train_base = pd.read_csv('./datasets/convenios_train.tsv.gz', compression='gzip', sep='\\t', encoding='utf-8')\n",
    "        X_train = train_base.drop([ylabel_name], axis=1)\n",
    "        y_train = train_base[[ylabel_name]]\n",
    "        result += [X_train, y_train]\n",
    "\n",
    "    if type.lower() in ['test', 'both']:\n",
    "        test_base = pd.read_csv('./datasets/convenios_test.tsv.gz', compression='gzip', sep='\\t', encoding='utf-8')\n",
    "        X_test = test_base.drop([ylabel_name], axis=1)\n",
    "        y_test = test_base[[ylabel_name]]\n",
    "        result += [X_test, y_test]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3348bf73-2fdc-4d37-935d-29b98695b7c3",
   "metadata": {},
   "source": [
    "## 11. Criação das bases de treino e de teste\n",
    "> Executado apenas quando se quer criar/recriar as bases de treino e teste a partir dos dados originais (arquivo 'convenios.txt').  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8f5383-8920-4bb4-b92b-1b3a1da3bc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_train_test_bases(convenios=convenios, proponentes=proponentes, emendas=emendas, emendas_convenios=emendas_convenios, fornecedores=fornecedores, movimento=movimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a8f9c-ca79-4545-91cc-91fb87ff8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "Liberação de memória com garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fc9b97-e85d-41a7-aca7-bfe7cfe8d9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del convenios\n",
    "del proponentes\n",
    "del emendas_convenios\n",
    "del emendas\n",
    "del movimento\n",
    "del fornecedores \n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9609dd41-6a80-43d1-be89-3134f3f85ae7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 12. Carregamento da base de treino e preparação dos dados \n",
    "> A função load_bases é utilizado para carregar a base de treino. Em seguida:  \n",
    "> 1. A função make_transformers é utilizada para preparar os trasnformadores (normalizadores e codificadores). \n",
    ">\n",
    "> 2. Também é instanciado um objeto PCA (principal component analysis) cuja finalidade é realizar a redução da dimensionalidade do modelo por meio do conceito de autovalores e autovetores.\n",
    ">\n",
    "> 3. Por fim, são aplicadas a preparação dos dados e a transformação PCA, de modo a se obter um cujunto de características (features) adquado e suficiente ao processo de teinamento do classificador.  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03c1079-0eeb-4ed1-a47e-de0d847818a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train and test bases loading\n",
    "X_train, y_train = load_bases(type='train')\n",
    "\n",
    "#data preparation\n",
    "transformers = make_transformers(X_train, stop_words_path='./datasets/stopwords.txt.gz', accented_path='./datasets/accented.txt.gz')\n",
    "pca = PCA(n_components=700)\n",
    "\n",
    "X_train = data_preparation(transformers, X_train)\n",
    "pca.fit(X_train, y_train)\n",
    "X_train = pca.transform(X_train)\n",
    "print(f'Explained Variance Ratio ---> {round(sum(pca.explained_variance_ratio_)*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786ef255-48d7-48ef-a000-18537c168d43",
   "metadata": {},
   "source": [
    "## 13. Seleção do algorítimo e ajuste dos hiperparâmetros (tuning)\n",
    "> Cada algorítimo candidato é treinado e avaliado com cross-validation por meio da função get_tuned_estimator, que utiliza o GridSearchCV. Neste caso o score a ser observado é o da precisão, já que se pretende que as predições de convênios com insucesso tenham mais probabilidade de estarem corretas (precision/precisão), mesmo que não se tenha bom desempenho em identificar todos convênios com insucesso (recall/revocação).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9966fecc-e509-4f46-a142-48472df90657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimators_list = [\n",
    "    {\n",
    "     'name': 'GNB',\n",
    "     'estimator': GaussianNB(),\n",
    "     'param_dist': {\n",
    "             'var_smoothing': [1e-12, 1e-15, 1e-20]\n",
    "              }\n",
    "\n",
    "    },\n",
    "\n",
    "    {\n",
    "     'name': 'DTR',\n",
    "     'estimator': DecisionTreeClassifier(),\n",
    "     'param_dist': {\n",
    "             'criterion': ['gini', 'entropy'],\n",
    "              }\n",
    "\n",
    "    },\n",
    "\n",
    "    \n",
    "    {\n",
    "     'name': 'SVC',\n",
    "     'estimator': SVC(),\n",
    "     'param_dist': { \n",
    "                 'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "                 'gamma':[2], \n",
    "                 'C': [1]\n",
    "                 }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "     'name': 'ADA',\n",
    "     'estimator': AdaBoostClassifier(),\n",
    "     'param_dist': {\n",
    "             'n_estimators': [25, 50],\n",
    "             'learning_rate': [1.0],\n",
    "              }\n",
    "\n",
    "    },\n",
    "    \n",
    "    {\n",
    "     'name': 'MLP',\n",
    "     'estimator': MLPClassifier(),\n",
    "     'param_dist': {\n",
    "                  'hidden_layer_sizes': [(10,), (20,), (30,), (10, 10), (10, 20), (10, 30)],\n",
    "                  'alpha': [0.0001], \n",
    "                  'shuffle': [False],\n",
    "                  'activation': ['relu'],\n",
    "                  'max_iter': [2000]\n",
    "                  }\n",
    "    },\n",
    "    \n",
    "    {\n",
    "     'name': 'RFC',\n",
    "     'estimator': RandomForestClassifier(),\n",
    "     'param_dist': {\n",
    "                  'n_estimators': [25, 50],\n",
    "                  'criterion': ['gini', 'entropy'],\n",
    "                  'max_features': [None]\n",
    "                  }\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfeb58-2b76-48c2-bf3b-afb73e341803",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = 'recall'\n",
    "classifiers = {}\n",
    "best_estimators = pd.DataFrame(data=[], columns=['estimator', 'params', scoring])\n",
    "\n",
    "for est in estimators_list:\n",
    "    print(f\"estimator: {est['name']} --> \", end='')\n",
    "    gs = get_tuned_estimator(X_train, y_train, estimator=est['estimator'], param_dist=est['param_dist'], scoring=scoring)\n",
    "    params, score = gs.best_params_, gs.best_score_\n",
    "    best_estimators = pd.concat([best_estimators, pd.DataFrame([[est['name'], str(params.copy())*3, score]], columns=best_estimators.columns)], axis=0, ignore_index=True, sort=False)\n",
    "    classifiers[est['name']] = (gs.best_estimator_, params.copy())\n",
    "    print(f'{scoring} = {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e9778-9154-4a92-be85-a1072aa05c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estimators.plot.bar(x='estimator', y=scoring, logy=False, ylim=[0, 1], grid=True, yticks=np.arange(0.05, 1, 0.05), figsize=(14, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ba5d4-d23b-44bc-ad5c-22b698da3cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(best_estimators.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6e0d0-98cd-404b-a970-d92920e2fc10",
   "metadata": {},
   "source": [
    "\n",
    "## 14. Avaliação dos classificadores selecionados  \n",
    "> A finalidade aqui é comparar o desempenho dos classificadores configurados com os hiperparâmetros selecionados, na base de teste.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bc110-172f-442a-a67b-3c38c1087926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "X_test, y_test = load_bases(type='test')\n",
    "X_test = data_preparation(transformers, X_test)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "for clf in classifiers:\n",
    "    p = classifiers[clf][0].predict(X_test)\n",
    "    print(f'Relatório de classificação para {clf}', '\\n')\n",
    "    print(classification_report(y_test['INSUCESSO'], p))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23806105-38fe-4326-9880-e2b1233f667b",
   "metadata": {},
   "source": [
    "## 15. Escolha do classificador\n",
    "> Escolha baseada nas métricas recall, precision, f1-score e accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f498d5-d7a3-434c-a430-9df3a76c16bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_name = 'GNB'\n",
    "selected_classifier = classifiers[clf_name][0]\n",
    "print(f'{clf_name} - Hiperparâmetros: {classifiers[clf_name][1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653d58f-d470-4981-ae53-58d3f5dc433a",
   "metadata": {},
   "source": [
    "## 16. Armazenamento do modelo treinado em formato pickle\n",
    "> A finalidade é que o modelo treinado possa ser utilizado em outra aplicação a partir do carregamento do arquivo pickle.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11e03a-7216-455a-ba78-6249c1b1dd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(object_model, filename):\n",
    "    with open(filename, 'wb') as fd:\n",
    "        pickle.dump(object_model, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798fd6ff-171a-40a2-aa0d-ab92217070a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModel(object):\n",
    "    def __init__(self, transformers, pca, clf):\n",
    "        self.transformers = transformers\n",
    "        self.pca = pca\n",
    "        self.clf = selected_classifier\n",
    "        \n",
    "save_model(MLModel(transformers, pca, clf), './trained_model/model.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
