{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0fdd5c-6af2-4004-9ed7-fca44a158463",
   "metadata": {},
   "source": [
    "## 1. Caregamento dos dados brutos\n",
    "> Esses estão sendo carregados apenas para realização de testes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5de0df15-34e0-413d-b6e8-9c2aa98aad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "convenios = pd.read_csv('../data_files/csv_files/convenios.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "proponentes = pd.read_csv('../data_files/csv_files/proponentes.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "emendas_convenios = pd.read_csv('../data_files/csv_files/emendas_convenios.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "emendas = pd.read_csv('../data_files/csv_files/emendas.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "movimento = pd.read_csv('../data_files/csv_files/movimento.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')\n",
    "fornecedores = pd.read_csv('../data_files/csv_files/fornecedores.csv.gz', decimal=',', sep=';', compression='gzip', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fdd6f-f7f2-4a6f-a778-b83301029db2",
   "metadata": {},
   "source": [
    "## 2. Declaração da classe TextTransformer  \n",
    "> Consiste em uma classe que convert textos em agrupamentos (clusters) por meio do algorítimo de clusterização k-means. Esta classe pode ser utilizada no tratamento de características (features) textuais não categóricas. No caso desta modelagem, será aplicada na feature 'OBJETO_PROPOSTA'. Como resultado, a feature será convertida em uma feature categórica não ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953b69ea-ea46-41f2-a24e-e7195816dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "        \n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, n_clusters=5, stop_words=[], accented=[]):\n",
    "        \n",
    "        #nltk.download('rslp', quiet=True)\n",
    "        self.__n_clusters__ = n_clusters\n",
    "        self.__stop_words__ = stop_words\n",
    "        self.__accented__ = accented\n",
    "        self.labels_ = None\n",
    "        self.__vectorizer__ = TfidfVectorizer(use_idf=True)\n",
    "        self.__clusterer__ = KMeans(n_clusters=self.__n_clusters__, random_state=0)\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.fit_transform(X)\n",
    "        self.__clusterer__ = self.__clusterer__.fit(X)\n",
    "        self.labels_ = self.__clusterer__.labels_\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.fit_transform(X)\n",
    "        X = self.__clusterer__.fit_transform(X)\n",
    "        self.labels_ = self.__clusterer__.labels_\n",
    "        return X\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.fit_transform(X)\n",
    "        y = self.__clusterer__.fit_predict(X)\n",
    "        self.labels_ = self.__clusterer__.labels_\n",
    "        return y\n",
    "    \n",
    "    def transform(self, X):        \n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.transform(X)\n",
    "        X = self.__clusterer__.transform(X)\n",
    "        return X\n",
    "                       \n",
    "    def predict(self, X):\n",
    "        \n",
    "        return_unique = False\n",
    "        if type(X)==str:\n",
    "            X = [X]\n",
    "            return_unique = True\n",
    "        \n",
    "        X = self.__preprocessing__(X)\n",
    "        X = self.__vectorizer__.transform(X)\n",
    "        y = self.__clusterer__.predict(X)\n",
    "        y = y[0] if return_unique else y\n",
    "        return y\n",
    "    \n",
    "    def __preprocessing__(self, X):\n",
    "        \n",
    "        X = [' '.join(self.__text_preprocessing__(t)) for t in X]\n",
    "        return X\n",
    "    \n",
    "    def __stemming__(self, tokens):  \n",
    "        \n",
    "        stemmer = nltk.stem.RSLPStemmer()\n",
    "        return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def __remove_accents__(self, text):\n",
    "        \n",
    "        for idx in self.__accented__.index:\n",
    "            text = text.replace(self.__accented__['char_acc'][idx], self.__accented__['char_norm'][idx]) \n",
    "        return text\n",
    "\n",
    "    def __text_preprocessing__(self, text):\n",
    "        \n",
    "        text = text.lower()\n",
    "        text = self.__remove_accents__(text)\n",
    "        tokens = re.findall('[a-z]+', text)\n",
    "        tokens = filter(lambda w: w is not None, map(lambda w: None if w in self.__stop_words__ or len(w)==1 else w , tokens))\n",
    "        tokens = self.__stemming__(tokens)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02964be0-c591-4413-9e0a-c780858348c4",
   "metadata": {},
   "source": [
    "## 3. Delcaração da classe RiskAnalyzer\n",
    "> A classe RiskAnalyzer é responsável por carregar o modelo de aprendizado de máquina (modelo) previamente treinado e armazenado em um arquivo no formato 'pickle'. Também tem a capacidade de realizar o treinamento do modelo e armazenado no arquivo para posterior carregamento. \n",
    "> \n",
    "> Essa classe pode gerar os datasets de treino e de teste a partir de um arquivo txt, separado por tab, previamente tratado para conter apenas as features de interesse. Essa classe também pode realizar essa operação a partir dos dados brutos disponíveis na modelo de dados, sem a necessidade do arquivo tratado. \n",
    "> \n",
    "> Também são declaradas as classes Metrics e MLModel. A primeira registra as metricas de avaliação de performance do modelo. Essa classe é instanciada duas vezes na RiskAnalyzer, uma para a validação cruzada (cross-validation) e outra para os testes (validation). A segunda classe (MLModel) armazena o classificador treinado e os objetos necessários para executar o pipeline de preparação dos dados para o treinamento ou a predição, além das métricas aferidas durante o treinamento.\n",
    "> \n",
    "> O algorítimo utilizado é o Classificador Random Forest que apresentou melhor desempenho dentre os algorítimos testados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28e036b-7e7b-41ef-b460-ac6995b7c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, cross_val_predict, StratifiedKFold\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "#from text_transformer import TextTransformer\n",
    "\n",
    "class MLModel(object):\n",
    "    \n",
    "    def __init__(self, transformers, principal_components_analysis, classifier, dimensionality_reduction_metrics, cross_validation_metrics, validation_metrics):\n",
    "        \n",
    "        self.transformers = transformers\n",
    "        self.principal_components_analysis = principal_components_analysis\n",
    "        self.classifier = classifier\n",
    "        self.validation_metrics = validation_metrics\n",
    "        self.cross_validation_metrics = cross_validation_metrics\n",
    "        self.dimensionality_reduction_metrics = dimensionality_reduction_metrics\n",
    "        \n",
    "        \n",
    "class Metrics(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.precision_score = None\n",
    "        self.recall_score = None\n",
    "        self.f1_score = None\n",
    "        self.accuracy_score = None\n",
    "        self.classification_report = None\n",
    "        \n",
    "    def json(self):\n",
    "        \n",
    "        metrics_json = self.__dict__.copy()\n",
    "        metrics_json.pop('classification_report')\n",
    "\n",
    "        return metrics_json\n",
    "        \n",
    "class DimensionalityReductionMetrics(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.original_dimensinality = None\n",
    "        self.reduced_dimensinality = None\n",
    "        self.explained_variance_ratio = None\n",
    "        \n",
    "    def json(self):\n",
    "        \n",
    "        metrics_json = self.__dict__.copy()\n",
    "\n",
    "        return metrics_json     \n",
    "        \n",
    "class RiskAnalyzer(object):\n",
    "    \n",
    "    def __init__(self, chunk_size=None, cross_validation_folds=10, ylabel_name = 'INSUCESSO',\n",
    "                       raw_dataset_path='.../etl/datasets/convenios.txt.gz', \n",
    "                       train_dataset_path='../etl/datasets/convenios_train.tsv.gz', \n",
    "                       test_dataset_path='../etl/datasets/convenios_test.tsv.gz'):\n",
    "        \n",
    "        self.cross_validation_folds = cross_validation_folds\n",
    "        self.chunk_size = chunk_size\n",
    "        self.raw_dataset_path = raw_dataset_path\n",
    "        self.ylabel_name = ylabel_name\n",
    "        self.pca_components = 700\n",
    "        \n",
    "        self.__model_filename_path__ = '../etl/trained_model/model.pickle'\n",
    "        self.__train_dataset_path__ = train_dataset_path\n",
    "        self.__test_dataset_path__ = test_dataset_path\n",
    "        self.__accented__ = '../etl/datasets/accented.txt.gz'\n",
    "        self.__stop_words__ = '../etl/datasets/stopwords.txt.gz'\n",
    "        \n",
    "        self.__model_object__ = self.__load_model__()\n",
    "\n",
    "        self.__ibge__ = None\n",
    "        self.__principais_parlamentares__ = None\n",
    "        self.__principais_fornecedores__ = None\n",
    "        \n",
    "    def make_train_test_bases(self, **tables):\n",
    "    \n",
    "        assert not tables or set(tables.keys()) == {'convenios', 'proponentes', 'emendas', 'emendas_convenios', \n",
    "        'fornecedores', 'movimento'}, '''Named arguments, if provided, must be: convenios, emendas, emendas_convenios, fornecedores, movimento'''\n",
    "\n",
    "        if tables:\n",
    "            print('transforming tables into dataset ... ', end='')\n",
    "            data = self.__transform_dataset__(**tables, ylabel=True)\n",
    "            data = data.drop(['NR_CONVENIO'], axis=1)\n",
    "            print('ok')\n",
    "        else:\n",
    "            print('loading dataset ... ', end='')\n",
    "            data = pd.read_csv(self.raw_dataset_path, compression='gzip', sep='\\t', encoding='utf-8')\n",
    "            data = data.drop(['NR_CONVENIO'], axis=1)\n",
    "            print('ok')\n",
    "\n",
    "        print('balancing ... ', end='')\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "        q0 = len(data[data[self.ylabel_name]==0])\n",
    "        q1 = len(data[data[self.ylabel_name]==1])\n",
    "        q = q0 if q0<q1 else q1\n",
    "\n",
    "        X = pd.concat([data[data[self.ylabel_name]==0].iloc[0:q], data[data[self.ylabel_name]==1].iloc[0:q]], sort=False)\n",
    "        y = X[[self.ylabel_name]]\n",
    "        X = pd.concat([data[data[self.ylabel_name]==0].iloc[0:q], data[data[self.ylabel_name]==1].iloc[0:q]], sort=False)\n",
    "        rest = data[~data.index.isin(X.index)]\n",
    "        print('ok')\n",
    "\n",
    "        print('spliting ... ', end='')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\n",
    "        print('ok')\n",
    "        \n",
    "        print('saving train and test datasets ... ', end='')\n",
    "        X_train.to_csv(self.__train_dataset_path__, compression='gzip', sep='\\t', encoding='utf-8', index=False)\n",
    "        X_test.to_csv(self.__test_dataset_path__, compression='gzip', sep='\\t', encoding='utf-8', index=False)\n",
    "        print('ok')\n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        print('loading bases ... ', end='')\n",
    "        X_train, y_train = self.__load_bases__(type='train')\n",
    "        print('ok')\n",
    "        \n",
    "        print('making transformers ... ', end='')\n",
    "        self.__model_object__.transformers = self.__make_transformers__(X_train)\n",
    "        print('ok')\n",
    "        \n",
    "        print('preparing data ... ', end='')\n",
    "        X_train = self.__data_preparation__(X_train)\n",
    "        print('ok')\n",
    "        \n",
    "        print('reducing dimensionality with principal components analysis ... ', end='')\n",
    "        X_train = self.__dimensionality_reduction(X_train, y_train)\n",
    "        print('ok')\n",
    "        \n",
    "        print('training model ... ', end='')\n",
    "        self.__model_object__.classifier = RandomForestClassifier(criterion='gini', n_estimators=200, max_features='sqrt').fit(X_train, y_train[self.ylabel_name])\n",
    "        print('ok')\n",
    "                       \n",
    "        print('scoring cross-validation ... ', end='')\n",
    "        self.__cross_validation_test__(X_train, y_train)\n",
    "        print('ok')\n",
    "\n",
    "        print('testing model ... ', end='')\n",
    "        self.__validation_test__()\n",
    "        print('ok')\n",
    "\n",
    "        print('saving model ... ', end='')\n",
    "        self.__save_model__()\n",
    "        print('ok')\n",
    "         \n",
    "    def metrics(self, type='validation'):\n",
    "        \n",
    "        assert type in ['validation', 'cross-validation', 'dimensionality'], \"type must be 'validation' or 'cross-validation' or 'dimensionality'\"\n",
    "        \n",
    "        if type=='validation':\n",
    "            return self.__model_object__.validation_metrics\n",
    "        elif type=='cross-validation':\n",
    "            return self.__model_object__.cross_validation_metrics\n",
    "        else:\n",
    "            return self.__model_object__.dimensionality_reduction_metrics\n",
    "        \n",
    "    def predict(self, X_conv, proba=False, scale=False, append=False):\n",
    "        dataframe_type = True\n",
    "        if not isinstance(X_conv, pd.DataFrame):\n",
    "            X= pd.DataFrame(X_conv)\n",
    "            dataframe_type = False\n",
    "        else:\n",
    "            X = X_conv.copy()\n",
    "        \n",
    "        nr_convenio = None\n",
    "        if 'NR_CONVENIO' in X.columns:\n",
    "            nr_convenio = X.pop('NR_CONVENIO').to_frame()\n",
    "            \n",
    "        X = self.__data_preparation__(X)\n",
    "        X = self.__model_object__.principal_components_analysis.transform(X)\n",
    "        if proba:\n",
    "            predictions = self.__model_object__.classifier.predict_proba(X)[:, 1]\n",
    "            if scale:\n",
    "                predictions = np.array(list(map(lambda value: self.__sigmoid__(value), predictions)))\n",
    "        else:\n",
    "            predictions = self.__model_object__.classifier.predict(X)\n",
    "            \n",
    "        predictions = pd.Series(predictions, name=self.ylabel_name)\n",
    "        \n",
    "        if append:\n",
    "            predictions = pd.DataFrame(predictions)\n",
    "            predictions = pd.concat([X, predictions], axis=1, sort=False, ignore_index=False)\n",
    "\n",
    "            if isinstance(nr_convenio, pd.DataFrame):\n",
    "                predictions = pd.concat([nr_convenio, predictions], axis=1, sort=False, ignore_index=False)\n",
    "        \n",
    "        if not dataframe_type:\n",
    "            predictions = predictions.to_dict()\n",
    "            \n",
    "        return predictions \n",
    "    \n",
    "    def run(self, convenios, proponentes, emendas, emendas_convenios, fornecedores, movimento, append=False):\n",
    "        \n",
    "        convenios_ = self.__transform_dataset__(convenios, proponentes, emendas, emendas_convenios, fornecedores, movimento)\n",
    "        \n",
    "        return self.predict(convenios_, proba=True, scale=True, append=append)\n",
    "    \n",
    "    def __cross_validation_test__(self, X_train=None, y_train=None):\n",
    "        \n",
    "        cv = StratifiedKFold(n_splits = self.cross_validation_folds, shuffle = True, random_state=200)\n",
    "        cv_scores = cross_validate(self.__model_object__.classifier, X_train, y_train[self.ylabel_name], cv=cv, \n",
    "                                   scoring=['precision', 'recall', 'f1', 'accuracy'])\n",
    "        \n",
    "        p = cross_val_predict(self.__model_object__.classifier, X_train, y_train[self.ylabel_name], cv=cv, method='predict')\n",
    "\n",
    "        cross_validation = Metrics()\n",
    "        cross_validation.precision_score = cv_scores['test_precision']\n",
    "        cross_validation.recall_score = cv_scores['test_recall']\n",
    "        cross_validation.f1_score = cv_scores['test_f1']\n",
    "        cross_validation.accuracy_score = cv_scores['test_accuracy']\n",
    "        cross_validation.classification_report = classification_report(y_train[self.ylabel_name], p)\n",
    "        \n",
    "        self.__model_object__.cross_validation_metrics = cross_validation\n",
    "        \n",
    "    def __validation_test__(self):\n",
    "        \n",
    "        X_test, y_test = self.__load_bases__(type='test')\n",
    "        \n",
    "        X_test = self.__data_preparation__(X_test)\n",
    "        X_test = self.__model_object__.principal_components_analysis.transform(X_test)\n",
    "\n",
    "        p = self.__model_object__.classifier.predict(X_test)\n",
    "        \n",
    "        validation = Metrics()\n",
    "        validation.precision_score = precision_score(y_test, p)\n",
    "        validation.recall_score = recall_score(y_test, p)\n",
    "        validation.f1_score = f1_score(y_test, p)\n",
    "        validation.accuracy_score = accuracy_score(y_test, p)\n",
    "        validation.classification_report = classification_report(y_test[self.ylabel_name], p)\n",
    "        \n",
    "        self.__model_object__.validation_metrics = validation\n",
    "    \n",
    "    def __make_transformers__(self, X):\n",
    "        \n",
    "        accented = pd.read_csv(self.__accented__, compression='gzip', sep=';', encoding='utf-8')\n",
    "        stop_words = pd.read_csv(self.__stop_words__, compression='gzip', encoding='utf-8', header=None)[0].tolist()\n",
    "        data = X.copy()\n",
    "        text_clusterer = TextTransformer(n_clusters=50, stop_words=stop_words, accented=accented).fit(data['OBJETO_PROPOSTA'])\n",
    "        data['OBJETO_PROPOSTA'] = text_clusterer.predict(data['OBJETO_PROPOSTA'])\n",
    "        data['OBJETO_PROPOSTA'] = data['OBJETO_PROPOSTA'].astype('int64')\n",
    "\n",
    "        data_categorical_parlamentar = data.pop('PRINCIPAL_PARLAMENTAR').to_frame()\n",
    "        data_categorical_fornecedor = data.pop('PRINCIPAL_FORNECEDOR').to_frame()\n",
    "\n",
    "        data_categorical_object = data.select_dtypes(include=['object'])\n",
    "        data_categorical_int = data.select_dtypes(include=['int64'])\n",
    "        data_value = data.select_dtypes(include='float64')\n",
    "\n",
    "        transformers = {}\n",
    "        transformers['TEXT_CLUSTERER'] = text_clusterer\n",
    "        transformers['VALUE'] = PowerTransformer().fit(data_value)\n",
    "        transformers['CATEGORICAL_OBJECT'] = OneHotEncoder(handle_unknown='ignore').fit(data_categorical_object)\n",
    "        transformers['CATEGORICAL_INT'] = OneHotEncoder(handle_unknown='infrequent_if_exist', max_categories=500).fit(data_categorical_int)\n",
    "        transformers['CATEGORICAL_PARLAMENTAR'] = OneHotEncoder(handle_unknown='infrequent_if_exist', max_categories=500).fit(data_categorical_parlamentar)\n",
    "        transformers['CATEGORICAL_FORNECEDOR'] = OneHotEncoder(handle_unknown='infrequent_if_exist', max_categories=500).fit(data_categorical_fornecedor)\n",
    "\n",
    "        return transformers\n",
    "    \n",
    "    def __data_preparation__(self, X):\n",
    "        \n",
    "        data = X.copy()\n",
    "        transformers = self.__model_object__.transformers\n",
    "        data['OBJETO_PROPOSTA'] = transformers['TEXT_CLUSTERER'].predict(data['OBJETO_PROPOSTA'])\n",
    "        data['OBJETO_PROPOSTA'] = data['OBJETO_PROPOSTA'].astype('int64')\n",
    "\n",
    "        data_categorical_parlamentar = data.pop('PRINCIPAL_PARLAMENTAR').to_frame()\n",
    "        data_categorical_fornecedor = data.pop('PRINCIPAL_FORNECEDOR').to_frame()\n",
    "\n",
    "        data_categorical_object = data.select_dtypes(include=['object'])\n",
    "        data_categorical_int = data.select_dtypes(include=['int64'])\n",
    "        data_value = data.select_dtypes(include='float64')\n",
    "\n",
    "        value_codes = transformers['VALUE'].transform(data_value)\n",
    "        value_feature_names = transformers['VALUE'].feature_names_in_\n",
    "        data_value = pd.DataFrame(value_codes, columns=value_feature_names).astype('float64')\n",
    "\n",
    "        categorical_object_codes = transformers['CATEGORICAL_OBJECT'].transform(data_categorical_object).toarray()\n",
    "        categorical_object_feature_names= transformers['CATEGORICAL_OBJECT'].get_feature_names_out()\n",
    "        data_categorical_object = pd.DataFrame(categorical_object_codes, columns=categorical_object_feature_names).astype('float64')\n",
    "\n",
    "        categorical_int_codes = transformers['CATEGORICAL_INT'].transform(data_categorical_int).toarray()\n",
    "        categorical_int_feature_names= transformers['CATEGORICAL_INT'].get_feature_names_out()\n",
    "        data_categorical_int = pd.DataFrame(categorical_int_codes, columns=categorical_int_feature_names).astype('float64')\n",
    "\n",
    "        parlamentar_codes = transformers['CATEGORICAL_PARLAMENTAR'].transform(data_categorical_parlamentar).toarray()\n",
    "        parlamentar_feature_names= transformers['CATEGORICAL_PARLAMENTAR'].get_feature_names_out()\n",
    "        data_categorical_parlamentar = pd.DataFrame(parlamentar_codes, columns=parlamentar_feature_names).astype('float64')\n",
    "\n",
    "        fornecedor_codes = transformers['CATEGORICAL_FORNECEDOR'].transform(data_categorical_fornecedor).toarray()\n",
    "        fornecedor_feature_names= transformers['CATEGORICAL_FORNECEDOR'].get_feature_names_out()\n",
    "        data_categorical_fornecedor = pd.DataFrame(fornecedor_codes, columns=fornecedor_feature_names).astype('float64')\n",
    "\n",
    "        return pd.concat([data_value, data_categorical_object, data_categorical_int,\n",
    "                          data_categorical_parlamentar, data_categorical_fornecedor], axis=1, sort=False)\n",
    "\n",
    "    def __dimensionality_reduction(self, X_train, y_train):\n",
    "        \n",
    "        dimensionality_metrics = DimensionalityReductionMetrics()\n",
    "        dimensionality_metrics.original_dimensinality = len(X_train.columns)\n",
    "        self.__model_object__.principal_components_analysis = PCA(n_components=self.pca_components)\n",
    "        X_train = self.__model_object__.principal_components_analysis.fit_transform(X_train, y_train)\n",
    "        dimensionality_metrics.reduced_dimensinality = len(X_train[0])\n",
    "        dimensionality_metrics.explained_variance_ratio = sum(self.__model_object__.principal_components_analysis.explained_variance_ratio_)\n",
    "        self.__model_object__.dimensionality_reduction_metrics = dimensionality_metrics\n",
    "        return X_train\n",
    "    \n",
    "    def __load_bases__(self, type='both'):\n",
    "        \n",
    "        assert type in ['train', 'test', 'both']\n",
    "        \n",
    "        result = []\n",
    "        if type.lower() in ['train', 'both']:\n",
    "            train_base = pd.read_csv(self.__train_dataset_path__, compression='gzip', sep='\\t', encoding='utf-8')\n",
    "            X_train = train_base.drop([self.ylabel_name], axis=1)\n",
    "            y_train = train_base[[self.ylabel_name]]\n",
    "            result += [X_train, y_train]\n",
    "\n",
    "        if type.lower() in ['test', 'both']:\n",
    "            test_base = pd.read_csv(self.__test_dataset_path__, compression='gzip', sep='\\t', encoding='utf-8')\n",
    "            X_test = test_base.drop([self.ylabel_name], axis=1)\n",
    "            y_test = test_base[[self.ylabel_name]]\n",
    "            result += [X_test, y_test]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __sigmoid__(self, value):\n",
    "        \n",
    "        middle_value = 0.5\n",
    "        slope_factor = 2\n",
    "        return 1/(1+np.e**(-slope_factor*(value-middle_value)))\n",
    "            \n",
    "    def __save_model__(self):\n",
    "        \n",
    "        with open(self.__model_filename_path__, 'wb') as fd:\n",
    "            pickle.dump(self.__model_object__, fd)\n",
    "    \n",
    "    def __load_model__(self):\n",
    "        \n",
    "        with open(self.__model_filename_path__, 'rb') as fd:\n",
    "            model_object = pickle.load(fd)\n",
    "        return model_object\n",
    "    \n",
    "    def __transform_dataset__(self, convenios, proponentes, emendas, emendas_convenios, fornecedores, movimento, ylabel=False):\n",
    "\n",
    "        self.__ibge__ = proponentes[['IDENTIF_PROPONENTE', 'CODIGO_IBGE']].copy()\n",
    "        \n",
    "        selected_columns = ['VL_REPASSE_CONV', 'VL_CONTRAPARTIDA_CONV', 'VALOR_EMENDA_CONVENIO',\n",
    "               'OBJETO_PROPOSTA', 'COD_ORGAO', 'COD_ORGAO_SUP', 'NATUREZA_JURIDICA',\n",
    "               'MODALIDADE', 'IDENTIF_PROPONENTE', 'COM_EMENDAS']\n",
    "        \n",
    "        features_columns = ['NR_CONVENIO', *selected_columns]\n",
    "        if ylabel:\n",
    "            features_columns += [self.ylabel_name]\n",
    "            \n",
    "        convenios_ = convenios[features_columns].copy()\n",
    "        \n",
    "        self.__principais_parlamentares__ = self.__get_principais_parlamentares__(emendas=emendas, emendas_convenios=emendas_convenios, convenios_list=convenios_['NR_CONVENIO'].to_list())\n",
    "        self.__principais_fornecedores__ = self.__get_principais_fornecedores__(movimento=movimento, fornecedores=fornecedores, convenios_list=convenios_['NR_CONVENIO'].to_list())\n",
    "\n",
    "        dataset = pd.merge(convenios_, self.__ibge__, how='inner', on=['IDENTIF_PROPONENTE'], left_index=False, right_index=False)\n",
    "\n",
    "        dataset = pd.merge(dataset, self.__principais_parlamentares__, how='left', on=['NR_CONVENIO'], left_index=False, right_index=False)\n",
    "\n",
    "        dataset = pd.merge(dataset, self.__principais_fornecedores__, how='left', on=['NR_CONVENIO'], left_index=False, right_index=False)\n",
    "\n",
    "        dataset = dataset.fillna('NAO APLICAVEL')\n",
    "        \n",
    "        Xdtypes = {'VL_REPASSE_CONV': 'float64', 'VL_CONTRAPARTIDA_CONV': 'float64', \n",
    "                   'VALOR_EMENDA_CONVENIO': 'float64', 'OBJETO_PROPOSTA': 'object', \n",
    "                   'COD_ORGAO': 'int64', 'COD_ORGAO_SUP': 'int64', 'NATUREZA_JURIDICA': 'object', \n",
    "                   'MODALIDADE': 'object', 'IDENTIF_PROPONENTE': 'int64', 'COM_EMENDAS': 'int64',\n",
    "                   'CODIGO_IBGE': 'int64', 'PRINCIPAL_PARLAMENTAR': 'object', 'PRINCIPAL_FORNECEDOR': 'object'}\n",
    "        \n",
    "        return dataset.astype(Xdtypes)\n",
    "    \n",
    "    def __get_principais_parlamentares__(self, emendas, emendas_convenios, convenios_list):\n",
    "        \n",
    "        convenios_repasses_emendas = emendas_convenios.loc[emendas_convenios['NR_CONVENIO'].isin(convenios_list)].copy()\n",
    "        convenios_repasses_emendas['rank'] = convenios_repasses_emendas.groupby(by=['NR_CONVENIO'])['VALOR_REPASSE_EMENDA'].rank(ascending=False, method='min')\n",
    "        convenios_repasses_emendas = convenios_repasses_emendas.loc[convenios_repasses_emendas['rank']==1, ['NR_CONVENIO', 'NR_EMENDA']]\n",
    "        convenios_parlamentares = pd.merge(convenios_repasses_emendas, emendas, on=['NR_EMENDA'], left_index=False, right_index=False)\n",
    "        convenios_parlamentares = convenios_parlamentares[['NR_CONVENIO', 'NOME_PARLAMENTAR']]\n",
    "        convenios_parlamentares.columns = ['NR_CONVENIO', 'PRINCIPAL_PARLAMENTAR']\n",
    "        convenios_parlamentares = convenios_parlamentares.groupby(by=['NR_CONVENIO']).max().reset_index()\n",
    "        \n",
    "        return convenios_parlamentares\n",
    "\n",
    "    def __get_principais_fornecedores__(self, movimento, fornecedores, convenios_list):\n",
    "        \n",
    "        movimento_exec = movimento.loc[movimento['NR_CONVENIO'].isin(convenios_list)].copy()\n",
    "        movimento_exec = movimento_exec[movimento_exec['TIPO_MOV']=='P']\n",
    "        convenios_fornecedores = movimento_exec[['NR_CONVENIO', 'FORNECEDOR_ID', 'VALOR_MOV']].groupby(by=['NR_CONVENIO', 'FORNECEDOR_ID']).sum().reset_index().copy()\n",
    "        convenios_fornecedores['rank'] = convenios_fornecedores.groupby(by=['NR_CONVENIO'])['VALOR_MOV'].rank(ascending=False, method='min')\n",
    "        convenios_fornecedores = convenios_fornecedores.loc[convenios_fornecedores['rank']==1, ['NR_CONVENIO', 'FORNECEDOR_ID']]\n",
    "        convenios_fornecedores = pd.merge(convenios_fornecedores, fornecedores, on=['FORNECEDOR_ID'], left_index=False, right_index=False)\n",
    "        convenios_fornecedores = convenios_fornecedores.sort_values(['NR_CONVENIO', 'IDENTIF_FORNECEDOR'], ascending=False)\n",
    "        convenios_fornecedores = convenios_fornecedores[['NR_CONVENIO', 'IDENTIF_FORNECEDOR']]\n",
    "        convenios_fornecedores.columns = ['NR_CONVENIO', 'PRINCIPAL_FORNECEDOR']\n",
    "        convenios_fornecedores = convenios_fornecedores.groupby(by=['NR_CONVENIO']).max().reset_index()\n",
    "        \n",
    "        return convenios_fornecedores\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "becffc59-94af-4cb4-8abb-88ed93d5e9f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../etl/trained_model/model.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ra \u001b[38;5;241m=\u001b[39m \u001b[43mRiskAnalyzer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mRiskAnalyzer.__init__\u001b[0;34m(self, chunk_size, cross_validation_folds, ylabel_name, raw_dataset_path, train_dataset_path, test_dataset_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__accented__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../etl/datasets/accented.txt.gz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__stop_words__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../etl/datasets/stopwords.txt.gz\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__model_object__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load_model__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__ibge__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__principais_parlamentares__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mRiskAnalyzer.__load_model__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__load_model__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__model_filename_path__\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[1;32m    345\u001b[0m         model_object \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fd)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_object\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../etl/trained_model/model.pickle'"
     ]
    }
   ],
   "source": [
    "ra = RiskAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7905bff-bbf5-41ba-b4a5-9a27e87c397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convenios_executados = convenios[(convenios['DIA_FIM_VIGENC_CONV'].astype('datetime64[ns]').dt.year >=2017) & (convenios['DIA_PUBL_CONV'].notna())].copy()\n",
    "convenios_executados = convenios_executados[convenios_executados['SIT_CONVENIO'].str.upper()!='EM EXECUÇÃO']\n",
    "convenios_executados['INSUCESSO'] = 0\n",
    "convenios_executados.loc[convenios_executados['SIT_CONVENIO'].isin(['Prestação de Contas Rejeitada', 'Inadimplente', 'Convênio Rescindido']), 'INSUCESSO'] = 1\n",
    "ra.make_train_test_bases(convenios=convenios_executados, proponentes=proponentes, emendas=emendas, emendas_convenios=emendas_convenios, fornecedores=fornecedores, movimento=movimento)\n",
    "#ra.make_train_test_bases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72ed2dd-f7da-4713-b934-e15e8b892742",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15d79a-a47a-43a2-87d4-aedeea7f27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cross-validation classification report')\n",
    "print(ra.metrics(type='cross-validation').classification_report)\n",
    "print('\\n.......................................................\\n')\n",
    "print('Validation classification report')\n",
    "print(ra.metrics(type='validation').classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8771b-fb92-485c-8593-d26c21c88249",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_json = ra.metrics(type='cross-validation').json()\n",
    "metrics_json = dict(folds=np.arange(1, ra.cross_validation_folds+1, 1), **metrics_json)\n",
    "df_cv = pd.DataFrame(metrics_json)\n",
    "\n",
    "df_val = pd.DataFrame(data=[list(ra.metrics().json().values())], columns=list(ra.metrics().json().keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75dc267-628d-4f08-9de7-bf89abcdb728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c387b8-b34f-4809-94f6-e6db2ba6cfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50996a0e-0f18-4ad1-97cd-e884b320f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv.plot.line(xlabel='folds', ylabel='valor', x='folds', logy=False, ylim=[0, 1], grid=True, xticks=np.arange(1, 10, 1), yticks=np.arange(0.05, 1, 0.05), figsize=(10, 6))\n",
    "df_val.plot.bar(xlabel='métricas', ylabel='valor', logy=False, ylim=[0, 1], grid=True, yticks=np.arange(0.05, 1, 0.05), figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b323714",
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = ra.run(convenios_em_execucao, proponentes, emendas, emendas_convenios, fornecedores, movimento, append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b412aed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "risks.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be309f7-ef37-4c4e-a12d-b6ca2147849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convenios['INSUCESSO'] = 0\n",
    "convenios.loc[convenios['SIT_CONVENIO'].isin(['Prestação de Contas Rejeitada', 'Inadimplente', 'Convênio Rescindido']), 'INSUCESSO'] = 1\n",
    "convenios_em_execucao = convenios.loc[convenios['SIT_CONVENIO'].str.upper()=='EM EXECUÇÃO']\n",
    "convenios.loc[convenios['NR_CONVENIO'].isin(convenios_em_execucao['NR_CONVENIO'].to_list()), \n",
    "    ['INSUCESSO']] = ra.run(convenios_em_execucao, proponentes, emendas, emendas_convenios, fornecedores, movimento, append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e68df-f36f-4a02-98e5-87a6851a1045",
   "metadata": {},
   "outputs": [],
   "source": [
    "convenios.loc[convenios['NR_CONVENIO'].isin(convenios_em_execucao['NR_CONVENIO'].to_list())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6de08a-e2a7-4697-bd22-7243e996f8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "convenios.loc[convenios['NR_CONVENIO'].isin(convenios_em_execucao['NR_CONVENIO'].to_list()), \n",
    "    'INSUCESSO'] = risks.to_list()\n",
    "convenios[convenios['INSUCESSO'].isna()]\n",
    "#convenios.loc[(convenios['SIT_CONVENIO'].str.upper()=='EM EXECUÇÃO') & (convenios['INSUCESSO'].notna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a534404b",
   "metadata": {},
   "outputs": [],
   "source": [
    "convenios.loc[convenios['NR_CONVENIO'].isin(convenios_em_execucao['NR_CONVENIO'].to_list())]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
